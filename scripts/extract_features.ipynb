{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.io import imread\n",
    "\n",
    "from canon.pattern import LatentExtractor, RescaleExtractor\n",
    "from canon.util import split_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img_files, model_name):\n",
    "    if model_name == \"rescale\":\n",
    "        extractor = RescaleExtractor((16, 16))\n",
    "    else:\n",
    "        extractor = LatentExtractor(model_name)\n",
    "    img_data = np.array([imread(f) for f in img_files])\n",
    "    img_idx = np.array([[int(f[-9:-4])] for f in img_files])\n",
    "    codes = extractor.features(img_data)\n",
    "    return np.hstack([img_idx, codes])\n",
    "\n",
    "def extract_features(jpg_dir, model_name):\n",
    "    dir_path = os.path.join(\"img\", jpg_dir)\n",
    "    filenames = [os.path.join(dir_path, filename) for filename in os.listdir(dir_path)\n",
    "                 if (not filename[0] == '.') and filename[-4:] == \".jpg\"]\n",
    "    print('Found %d files in the directory %s.' % (len(filenames), dir_path))\n",
    "\n",
    "    fn_grps = split_workload(filenames, 32)\n",
    "\n",
    "    with Parallel(n_jobs=-1, verbose=1) as parallel:\n",
    "        data = parallel(delayed(process_img)(grp, model_name) for grp in fn_grps)\n",
    "        data = np.vstack(data)\n",
    "        data = data[data[:,0].argsort()]\n",
    "        print(\"Loaded a data of shape {}\".format(data.shape))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from BTO_01_m_zoom using ae_conv_4_256_best ...\n",
      "Found 5712 files in the directory img\\BTO_01_m_zoom.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a data of shape (5712, 257)\n",
      "Save 1 datasets to BTO_01_m_zoom.hdf5\n",
      "ae_conv_4_256_best (5712, 257)\n",
      "Final datasets in BTO_01_m_zoom.hdf5: ['ae_conv_4_256_best']\n",
      "29.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:   29.1s finished\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# jpg_dir = \"C5_30x30\"\n",
    "jpg_dir = \"BTO_01_m_zoom\"\n",
    "\n",
    "models = ['ae_conv_4_256_best']\n",
    "\n",
    "features = []\n",
    "for model in models:\n",
    "    print(\"Extracting features from {} using {} ...\".format(jpg_dir, model))\n",
    "    data = extract_features(jpg_dir, model)\n",
    "    features.append(data)\n",
    "\n",
    "hdf5_file = jpg_dir +'.hdf5'\n",
    "print(\"Save {} datasets to {}\".format(len(features), hdf5_file))\n",
    "with h5py.File(os.path.join(\"features\", hdf5_file), 'w') as h5f:\n",
    "    for m, d in zip(models, features):\n",
    "        print(m, d.shape)\n",
    "        h5f.create_dataset(m, data=d)\n",
    "with h5py.File(os.path.join(\"features\", hdf5_file), 'r') as h5f:\n",
    "    print(\"Final datasets in {}: {}\".format(hdf5_file, list(h5f.keys())))\n",
    "    \n",
    "print(\"%.2f\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae_conv_4_256_best\n",
      "(5712, 257)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(os.path.join(\"features\", hdf5_file), 'r') as h5f:\n",
    "    for k in h5f.keys():\n",
    "        print(k)\n",
    "        data = np.array(h5f.get(k))\n",
    "        print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
